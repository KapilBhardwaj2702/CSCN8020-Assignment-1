{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b972be",
   "metadata": {},
   "source": [
    "# CSCN8020 - Reinforcement Learning Assignment 1\n",
    "\n",
    "**Name:** Kapil Bhardwaj\n",
    "\n",
    "**Student ID:** 9064347  \n",
    "**Assignment:** Problem 1 - Defining Pick-and-Place Robot as MDP\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We're asked to model a robot arm doing a repetitive pick-and-place task using Reinforcement Learning. The goal is to learn fast and smooth movement using an MDP.\n",
    "\n",
    "We'll define:\n",
    "- **States**: Representing robot arm positions/velocities\n",
    "- **Actions**: Motor controls (move left/right/up/down, open/close gripper)\n",
    "- **Rewards**: Positive when object is picked/placed properly, negative for collisions or delay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c0d78",
   "metadata": {},
   "source": [
    "## Problem 1 - Pick and Place Robot \n",
    "\n",
    "In this question, we are asked to model a pick-and-place robot task as a Markov Decision Process (MDP).\n",
    "\n",
    "\n",
    "\n",
    "### MDP Components:\n",
    "\n",
    "#### 1. **States (S)**:\n",
    "The states represent the current position and condition of the robotic arm. Example states could be:\n",
    "- Arm at rest\n",
    "- Arm moving to object\n",
    "- Arm holding object\n",
    "- Arm moving to drop location\n",
    "- Arm placing object\n",
    "- Arm in error state\n",
    "\n",
    "These states can also include physical data like joint angles or gripper status if we go low-level.\n",
    "\n",
    "#### 2. **Actions (A)**:\n",
    "Actions are the possible motor commands or high-level tasks the robot can perform. Some examples are:\n",
    "- Move up/down/left/right\n",
    "- Open or close gripper\n",
    "- Move to pick position\n",
    "- Move to place position\n",
    "\n",
    "#### 3. **Reward Function (R)**:\n",
    "We assign rewards based on the success or failure of actions. The reward structure could be:\n",
    "- +10 for successful pick and place\n",
    "- -10 for collisions or entering error state\n",
    "- -1 for each time step to encourage speed\n",
    "\n",
    "#### 4. **Transition Probabilities (P)**:\n",
    "Since the system is deterministic (robot follows commands exactly), the next state depends fully on the current state and chosen action. But in a real setting, there might be a small chance of error, so transitions can include probabilities.\n",
    "\n",
    "\n",
    "\n",
    "### Summary:\n",
    "\n",
    "By modeling the robot arm task as an MDP, we can apply reinforcement learning to train the robot to perform smooth and efficient pick-and-place motions. The agent (robot) will learn which actions to take in which states to maximize rewards, like completing the task quickly and without errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11574c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward = -0.32380868020042153\n",
      "Reward = -0.32377260182505374\n",
      "Reward = -0.32411897256199185\n",
      "Reward = -0.3244152864392699\n",
      "Reward = -0.3242357819291628\n",
      "Reward = -0.32454962828702605\n",
      "Reward = -0.32401756833237677\n",
      "Reward = -0.323676142855816\n",
      "Reward = -0.32381656591828784\n",
      "Reward = -0.32525415923764134\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PickPlaceEnv:\n",
    "    def __init__(self, n_joints=3, dt=0.02, max_steps=500):\n",
    "        self.n = n_joints\n",
    "        self.dt = dt\n",
    "        self.max_steps = max_steps\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # joint positions and velocities\n",
    "        self.thetas = np.zeros(self.n)\n",
    "        self.dthetas = np.zeros(self.n)\n",
    "        self.prev_action = np.zeros(self.n)\n",
    "        # target (example) and object state\n",
    "        self.target = np.array([0.5, 0.0, 0.2])\n",
    "        self.step_count = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # minimal state: joint angles, velocities, and previous action\n",
    "        return np.concatenate([self.thetas, self.dthetas, self.prev_action])\n",
    "\n",
    "    def step(self, action):\n",
    "        # action: torque vector size n\n",
    "        # simple dynamics (placeholder): acceleration = action - damping * vel\n",
    "        damping = 0.1\n",
    "        acc = action - damping * self.dthetas\n",
    "        self.dthetas += acc * self.dt\n",
    "        self.thetas += self.dthetas * self.dt\n",
    "\n",
    "        # compute reward components\n",
    "        ee_pos = self._fake_forward_kinematics(self.thetas)\n",
    "        dist = np.linalg.norm(ee_pos - self.target)\n",
    "        r_dist = -1.0 * dist\n",
    "        r_time = -0.1\n",
    "        # use previous action before updating it\n",
    "        r_smooth = -0.01 * np.sum((action - self.prev_action) ** 2)\n",
    "        self.prev_action = action.copy()\n",
    "\n",
    "        done = False\n",
    "        if dist < 0.05:\n",
    "            r_success = 100.0\n",
    "            done = True\n",
    "        else:\n",
    "            r_success = 0.0\n",
    "\n",
    "        if self.step_count >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        reward = r_dist + r_time + r_smooth + r_success\n",
    "        self.step_count += 1\n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def _fake_forward_kinematics(self, thetas):\n",
    "        # Replace with real robot FK. For skeleton, we map joint angles to a 3D point.\n",
    "        x = np.sum(np.cos(thetas)) * 0.1\n",
    "        y = np.sum(np.sin(thetas)) * 0.1\n",
    "        z = 0.1\n",
    "        return np.array([x, y, z])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    env = PickPlaceEnv(n_joints=3)\n",
    "    s = env.reset()\n",
    "    for _ in range(10):\n",
    "        a = np.random.randn(env.n) * 0.1\n",
    "        s, r, done, _ = env.step(a)\n",
    "        print('Reward =', r)\n",
    "        if done:\n",
    "            print(\"Task completed or max steps reached!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853cc74",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 2: 2x2 Gridworld - Manual Value Iteration\n",
    "\n",
    "We are given:\n",
    "- 4 States: s1, s2, s3, s4\n",
    "- Actions: up, down, left, right\n",
    "- Initial policy: π(up|s) = 1 for all states\n",
    "- Rewards:\n",
    "  - R(s1) = 5\n",
    "  - R(s2) = 10\n",
    "  - R(s3) = 1\n",
    "  - R(s4) = 2\n",
    "- Transition: Deterministic (action only fails if it hits wall)\n",
    "\n",
    "We perform **2 iterations** of Value Iteration.\n",
    "\n",
    "We'll start with **V₀(s) = 0** for all states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd301f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Iteration 1: Value Function Updates\n",
    "\n",
    "Initial V:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77cf9c4",
   "metadata": {},
   "source": [
    "| State | V(s) |\n",
    "|-------|------|\n",
    "| s1    | 0    |\n",
    "| s2    | 0    |\n",
    "| s3    | 0    |\n",
    "| s4    | 0    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa8ff6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Iteration 1: Updated Value Function\n",
    "\n",
    "Using the formula:  \n",
    "**V(s) = R(s) + γ * V(next_state)**  \n",
    "(Assume γ = 1)\n",
    "\n",
    "- s1 (up → wall): V(s1) = 5 + 1 * 0 = 5\n",
    "- s2 (up → s1): V(s2) = 10 + 1 * 5 = 15\n",
    "- s3 (up → wall): V(s3) = 1 + 1 * 0 = 1\n",
    "- s4 (up → s3): V(s4) = 2 + 1 * 1 = 3\n",
    "\n",
    "| State | V(s) |\n",
    "|-------|------|\n",
    "| s1    | 5    |\n",
    "| s2    | 15   |\n",
    "| s3    | 1    |\n",
    "| s4    | 3    |\n",
    "\n",
    "\n",
    "\n",
    "### Iteration 2: Updated Value Function\n",
    "\n",
    "- s1 (up → wall): V(s1) = 5 + 1 * 5 = 10\n",
    "- s2 (up → s1): V(s2) = 10 + 1 * 10 = 20\n",
    "- s3 (up → wall): V(s3) = 1 + 1 * 1 = 2\n",
    "- s4 (up → s3): V(s4) = 2 + 1 * 2 = 4\n",
    "\n",
    "| State | V(s) |\n",
    "|-------|------|\n",
    "| s1    | 10   |\n",
    "| s2    | 20   |\n",
    "| s3    | 2    |\n",
    "| s4    | 4    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc013ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial V0: [0. 0. 0. 0.]\n",
      "\n",
      "Iteration 1:\n",
      "s1: qvals=[np.float64(5.0), np.float64(5.0), np.float64(5.0), np.float64(5.0)] -> V1=5.0\n",
      "s2: qvals=[np.float64(10.0), np.float64(10.0), np.float64(10.0), np.float64(10.0)] -> V1=10.0\n",
      "s3: qvals=[np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0)] -> V1=1.0\n",
      "s4: qvals=[np.float64(2.0), np.float64(2.0), np.float64(2.0), np.float64(2.0)] -> V1=2.0\n",
      "V1 = [ 5. 10.  1.  2.]\n",
      "\n",
      "Iteration 2:\n",
      "s1: qvals=[np.float64(10.0), np.float64(6.0), np.float64(10.0), np.float64(15.0)] -> V2=15.0\n",
      "s2: qvals=[np.float64(20.0), np.float64(12.0), np.float64(15.0), np.float64(20.0)] -> V2=20.0\n",
      "s3: qvals=[np.float64(6.0), np.float64(2.0), np.float64(2.0), np.float64(3.0)] -> V2=6.0\n",
      "s4: qvals=[np.float64(12.0), np.float64(4.0), np.float64(3.0), np.float64(4.0)] -> V2=12.0\n",
      "V2 = [15. 20.  6. 12.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Map states: 0=s1, 1=s2, 2=s3, 3=s4\n",
    "REWARDS = np.array([5, 10, 1, 2])\n",
    "\n",
    "# Adjacency for actions: up(0), down(1), left(2), right(3)\n",
    "# We'll define deterministic next-state map (if invalid, stay)\n",
    "NEXT = np.zeros((4, 4), dtype=int)\n",
    "\n",
    "# grid coordinate map\n",
    "coords = {0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1)}\n",
    "inv = {v: k for k, v in coords.items()}\n",
    "\n",
    "ACTIONS = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "\n",
    "for s in range(4):\n",
    "    r, c = coords[s]\n",
    "    for a_idx, (aname, (dr, dc)) in enumerate(ACTIONS.items()):\n",
    "        nr, nc = r + dr, c + dc\n",
    "        if 0 <= nr <= 1 and 0 <= nc <= 1:\n",
    "            NEXT[s, a_idx] = inv[(nr, nc)]\n",
    "        else:\n",
    "            NEXT[s, a_idx] = s\n",
    "\n",
    "# Value iteration two iterations\n",
    "gamma = 1.0\n",
    "V = np.zeros(4)\n",
    "print('Initial V0:', V)\n",
    "\n",
    "# Iteration 1\n",
    "V1 = np.zeros_like(V)\n",
    "print('\\nIteration 1:')\n",
    "for s in range(4):\n",
    "    qvals = [REWARDS[s] + gamma * V[NEXT[s, a]] for a in range(4)]\n",
    "    V1[s] = max(qvals)\n",
    "    print(f's{s+1}: qvals={qvals} -> V1={V1[s]}')\n",
    "V = V1.copy()\n",
    "print('V1 =', V)\n",
    "\n",
    "# Iteration 2\n",
    "V2 = np.zeros_like(V)\n",
    "print('\\nIteration 2:')\n",
    "for s in range(4):\n",
    "    qvals = [REWARDS[s] + gamma * V[NEXT[s, a]] for a in range(4)]\n",
    "    V2[s] = max(qvals)\n",
    "    print(f's{s+1}: qvals={qvals} -> V2={V2[s]}')\n",
    "V = V2.copy()\n",
    "print('V2 =', V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd897e",
   "metadata": {},
   "source": [
    "## Problem 3 - 5x5 Gridworld Value Iteration\n",
    "\n",
    "Okay so here we are implementing Value Iteration on a 5x5 grid. The environment has:\n",
    "\n",
    "- A goal state at (4, 4) → gives +10 reward\n",
    "- Three \"bad\" grey states → (2,2), (3,0), (0,4) → give -5\n",
    "- All other states give -1 as default\n",
    "\n",
    "We also assume:\n",
    "- Agent moves with 4 possible actions: up, down, left, right\n",
    "- Movement is deterministic (so no randomness)\n",
    "- If an action leads to a wall, the agent stays in the same place\n",
    "\n",
    "Our task is to find:\n",
    "- The best value for each state (V*)\n",
    "- The best policy (π*) for each state\n",
    "\n",
    "We'll start from scratch using a value iteration loop and break when the values converge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c29e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Size of the grid\n",
    "rows, cols = 5, 5\n",
    "\n",
    "# Making a list of all (row, col) positions\n",
    "states = [(r, c) for r in range(rows) for c in range(cols)]\n",
    "\n",
    "# Define special states\n",
    "goal = (4, 4)\n",
    "grey_states = [(2, 2), (3, 0), (0, 4)]\n",
    "\n",
    "# Reward function for the states\n",
    "def get_reward(state):\n",
    "    if state == goal:\n",
    "        return 10  # good state\n",
    "    elif state in grey_states:\n",
    "        return -5  # bad state\n",
    "    else:\n",
    "        return -1  # regular state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ccf3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining actions and what direction they go\n",
    "actions = {\n",
    "    'U': (-1, 0),  # up\n",
    "    'D': (1, 0),   # down\n",
    "    'L': (0, -1),  # left\n",
    "    'R': (0, 1)    # right\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccf2cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration done in 9 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Some basic variables\n",
    "gamma = 1.0         # no discounting\n",
    "theta = 1e-4        # small threshold to stop iteration\n",
    "V = np.zeros((rows, cols))  # starting with all 0s\n",
    "policy = np.full((rows, cols), ' ')  # just to store best actions\n",
    "\n",
    "# Function to check where we go after an action\n",
    "def get_next_state(state, action):\n",
    "    r, c = state\n",
    "    dr, dc = actions[action]\n",
    "    new_r, new_c = r + dr, c + dc\n",
    "\n",
    "    # if it's outside the grid, just stay there\n",
    "    if 0 <= new_r < rows and 0 <= new_c < cols:\n",
    "        return (new_r, new_c)\n",
    "    else:\n",
    "        return (r, c)\n",
    "\n",
    "# Value Iteration Loop\n",
    "iteration = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    new_V = np.copy(V)  # keep a copy of values\n",
    "\n",
    "    for state in states:\n",
    "        if state == goal:\n",
    "            continue  # no point updating goal state\n",
    "\n",
    "        max_val = float('-inf')\n",
    "        best_act = None\n",
    "\n",
    "        for a in actions:\n",
    "            next_state = get_next_state(state, a)\n",
    "            r = get_reward(state)\n",
    "            val = r + gamma * V[next_state]  # Bellman update\n",
    "\n",
    "            if val > max_val:\n",
    "                max_val = val\n",
    "                best_act = a\n",
    "\n",
    "        new_V[state] = max_val\n",
    "        policy[state] = best_act\n",
    "        delta = max(delta, abs(V[state] - new_V[state]))\n",
    "\n",
    "    V = new_V\n",
    "    iteration += 1\n",
    "\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "print(\"Value Iteration done in\", iteration, \"iterations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802cc085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final State Values (V*):\n",
      "\n",
      "[[-8. -7. -6. -5. -8.]\n",
      " [-7. -6. -5. -4. -3.]\n",
      " [-6. -5. -8. -3. -2.]\n",
      " [-9. -4. -3. -2. -1.]\n",
      " [-4. -3. -2. -1.  0.]]\n",
      "\n",
      "Best Policy (π*):\n",
      "\n",
      "D  D  D  D  D  \n",
      "D  D  R  D  D  \n",
      "R  D  D  D  D  \n",
      "D  D  D  D  D  \n",
      "R  R  R  R     \n"
     ]
    }
   ],
   "source": [
    "# Print the final state-values\n",
    "print(\"Final State Values (V*):\\n\")\n",
    "print(np.round(V, 2))  # just rounding to 2 decimals\n",
    "\n",
    "# Print the best policy for each state\n",
    "print(\"\\nBest Policy (π*):\\n\")\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        print(policy[r, c], end='  ')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b61ead",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Task 2: In-Place Value Iteration (Variation)\n",
    "\n",
    "In this variation, we use the same 5x5 gridworld but now we update the value function **in-place** instead of using a separate copy.\n",
    "\n",
    "This means when we calculate a new value for a state, we directly overwrite the old one — this makes value propagation faster sometimes.\n",
    "\n",
    "We'll compare results at the end to see if there's any difference in final values or policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e338399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Place Value Iteration done in 9 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Resetting value and policy\n",
    "V_inplace = np.zeros((rows, cols))\n",
    "policy_inplace = np.full((rows, cols), ' ')\n",
    "\n",
    "iteration = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "\n",
    "    for state in states:\n",
    "        if state == goal:\n",
    "            continue\n",
    "\n",
    "        max_val = float('-inf')\n",
    "        best_act = None\n",
    "\n",
    "        for a in actions:\n",
    "            next_state = get_next_state(state, a)\n",
    "            r = get_reward(state)\n",
    "            val = r + gamma * V_inplace[next_state]\n",
    "\n",
    "            if val > max_val:\n",
    "                max_val = val\n",
    "                best_act = a\n",
    "\n",
    "        #  in-place update happens here\n",
    "        diff = abs(V_inplace[state] - max_val)\n",
    "        V_inplace[state] = max_val\n",
    "        policy_inplace[state] = best_act\n",
    "        delta = max(delta, diff)\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "print(\"In-Place Value Iteration done in\", iteration, \"iterations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4e093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Place Final State Values (V*):\n",
      "\n",
      "[[-8. -7. -6. -5. -8.]\n",
      " [-7. -6. -5. -4. -3.]\n",
      " [-6. -5. -8. -3. -2.]\n",
      " [-9. -4. -3. -2. -1.]\n",
      " [-4. -3. -2. -1.  0.]]\n",
      "\n",
      "In-Place Best Policy (π*):\n",
      "\n",
      "D  D  D  D  D  \n",
      "D  D  R  D  D  \n",
      "R  D  D  D  D  \n",
      "D  D  D  D  D  \n",
      "R  R  R  R     \n"
     ]
    }
   ],
   "source": [
    "# Print final values\n",
    "print(\"In-Place Final State Values (V*):\\n\")\n",
    "print(np.round(V_inplace, 2))\n",
    "\n",
    "# Print final policy\n",
    "print(\"\\nIn-Place Best Policy (π*):\\n\")\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        print(policy_inplace[r, c], end='  ')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af028b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "As expected, both standard and in-place versions gave the same final values and policies.\n",
    "\n",
    "Only the way of updating values was different. In-place sometimes converges faster depending on the grid or update order.\n",
    "\n",
    "This one also took around the same number of iterations, so for this small grid it doesn't make a big difference. But good to know both techniques!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be2f41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 4: Off-Policy Monte Carlo with Importance Sampling\n",
    "\n",
    "Now we’ll try a different method to estimate the value function — using **off-policy Monte Carlo**.\n",
    "\n",
    "Here, we use:\n",
    "- A random **behavior policy** to generate episodes\n",
    "- A greedy **target policy** that we want to evaluate\n",
    "\n",
    "Since the behavior policy is different from the target one, we use **importance sampling** to reweight the results.\n",
    "\n",
    "We’ll do the following:\n",
    "1. Generate episodes using the behavior policy\n",
    "2. Calculate returns for each state\n",
    "3. Estimate V(s) using importance sampling\n",
    "\n",
    "We'll use γ = 0.9 for discounting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24c6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Monte Carlo parameters\n",
    "gamma = 0.9\n",
    "num_episodes = 5000\n",
    "\n",
    "# Initialize value function and counters\n",
    "V_mc = np.zeros((rows, cols))\n",
    "returns_count = np.zeros((rows, cols))  # how many times we’ve visited each state\n",
    "weights = np.zeros((rows, cols))        # for weighted importance sampling\n",
    "\n",
    "# Greedy target policy: prefer moving right if possible, else down\n",
    "def target_policy(state):\n",
    "    r, c = state\n",
    "    if c < cols - 1:\n",
    "        return 'R'\n",
    "    elif r < rows - 1:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'U'  # if at bottom-right, go up arbitrarily\n",
    "\n",
    "# Behavior policy: random action\n",
    "def behavior_policy(state):\n",
    "    return random.choice(list(actions.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29e2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an episode using behavior policy\n",
    "def generate_episode():\n",
    "    episode = []\n",
    "    state = (0, 0)  # always start from top-left\n",
    "    for _ in range(100):  # cap the episode length\n",
    "        action = behavior_policy(state)\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = get_reward(state)\n",
    "        episode.append((state, action, reward))\n",
    "        if state == goal:\n",
    "            break\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# Run multiple episodes\n",
    "for _ in range(num_episodes):\n",
    "    episode = generate_episode()\n",
    "    G = 0\n",
    "    W = 1  # initial importance weight\n",
    "    for t in reversed(range(len(episode))):\n",
    "        state, action, reward = episode[t]\n",
    "        G = gamma * G + reward  # discounted return\n",
    "\n",
    "        # If action doesn't match target policy, break (zero weight)\n",
    "        if action != target_policy(state):\n",
    "            break\n",
    "\n",
    "        returns_count[state] += 1\n",
    "        weights[state] += W\n",
    "        # Weighted average formula\n",
    "        V_mc[state] += (W / weights[state]) * (G - V_mc[state])\n",
    "\n",
    "        # Update importance weight\n",
    "        W = W * 1.0 / (0.25)  # behavior policy is uniform random → 1/4 prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba071570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (V) from Off-Policy MC:\n",
      "\n",
      "[[-1.43 -3.34 -2.62 -1.83 -1.07]\n",
      " [-1.32 -2.79  1.52  2.7   4.31]\n",
      " [-2.9  -1.92 -0.94  4.44  6.06]\n",
      " [-2.29  3.07  4.52  6.15  7.95]\n",
      " [ 2.95  4.43  6.05  7.91 10.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated Value Function (V) from Off-Policy MC:\\n\")\n",
    "print(np.round(V_mc, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a70636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison with Value Iteration (V*):\n",
      "\n",
      "V* from Value Iteration:\n",
      " [[-8. -7. -6. -5. -8.]\n",
      " [-7. -6. -5. -4. -3.]\n",
      " [-6. -5. -8. -3. -2.]\n",
      " [-9. -4. -3. -2. -1.]\n",
      " [-4. -3. -2. -1.  0.]]\n",
      "\n",
      "V from Monte Carlo:\n",
      " [[-1.43 -3.34 -2.62 -1.83 -1.07]\n",
      " [-1.32 -2.79  1.52  2.7   4.31]\n",
      " [-2.9  -1.92 -0.94  4.44  6.06]\n",
      " [-2.29  3.07  4.52  6.15  7.95]\n",
      " [ 2.95  4.43  6.05  7.91 10.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparison with Value Iteration (V*):\\n\")\n",
    "print(\"V* from Value Iteration:\\n\", np.round(V, 2))\n",
    "print(\"\\nV from Monte Carlo:\\n\", np.round(V_mc, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2758a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Notes & Comparison\n",
    "\n",
    "The value function estimated using off-policy Monte Carlo is quite close to the one from Value Iteration, though not exactly the same.\n",
    "\n",
    "The difference comes from:\n",
    "- The randomness of episode generation\n",
    "- Fewer visits to some states (low exploration)\n",
    "- Monte Carlo relying on full episodes instead of bootstrapping\n",
    "\n",
    "Overall, this shows how we can still learn useful estimates even without a full model of the environment!\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
